// Defines the mouselab meta-MDP and policies for solving it.

var utils = webpplMouselab;
var TERM_ACTION = '__TERM_ACTION__';
var TERM_STATE = '__TERM_STATE__';
var UNKNOWN = '__';
var INITIAL_NODE = 0

// The agent can observe an unobserved node or terminate observation.




// Defines a set of mouselab problems with the same structure.
// Returns a function `makeEnv(trueRewards)` that returns a mouselab
// problem with the given structure and the true hidden rewards
// in trueRewards
var mouselabProblem = function(params) {
  console.log('mouselab', params)

  // ============================== //
  // ========= Object MDP ========= //
  // ============================== //
  var objectEnv = utils.buildEnv();
  var tree = objectEnv.tree;
  var nodes = _.range(tree.length);

  // The expected reward at a node from the agent's perspective.
  var expectedNodeReward = function(state, node) {
    state[node] == UNKNOWN ?
    expectation(params.reward) :
    state[node]
  };

  var children = function(node) {
    tree[node]
  };

  var unobservedNodes = function(state) {
    filter(function(node) {state[node] == UNKNOWN}, nodes)
  };
  var numObserved = function(state) {
    filter(function(node) {state[node] != UNKNOWN}, nodes).length - 1
  };
  var actions = function(state) {
    unobservedNodes(state).concat([TERM_ACTION])
  };

  // Like a Q function, the expected value of reaching a node.
  // Includes reward at that node plus expected value of the node.
  var nodeQuality = dp.cache(function(state, node) {
    var best_child_val = (
      children(node).length == 0 ?
      0 :
      _.max(map(function(child) {nodeQuality(state, child)},
                children(node)))
    );
    return expectedNodeReward(state, node) + best_child_val
  }); 

  // For now, the agent deterministially receives the expected
  // termination reward when it terminates. Alternatievly,
  // this function could sample from the distribution of returns
  // when following the path with highest expected value.
  var termReward = function(state) {
    var expectedReward = nodeQuality(state, INITIAL_NODE);
    return expectedReward
  };

  // ============================ //
  // ========= Meta MDP ========= //
  // ============================ //
  var makeEnv = function(trueRewards) {

    // The true reward at a given state.
    var truth = trueRewards || [0].concat(repeat(tree.length-1, 
                                                 function() {sample(params.reward)}))
    var nodeReward = function(node) {
      return truth[node]
    };

    // Observing nodes and terminating observation.
    var transition = function(state, action) {
      if (state == TERM_STATE) error('transition from term ' + action);
      if (action == TERM_ACTION) return TERM_STATE;
      if (state[action] != UNKNOWN) {
        error('observing state twice\n' + JSON.stringify(state)+' '+action);
      }
      return utils.updateList(state, action, nodeReward(action));
    };

    // Cost of observation and reward for terminating.
    var reward = function(state, action) {
      (action == TERM_ACTION ?
       termReward(state) :
       params)
    };

    return {
      initialState: objectEnv.initialState,
      reward,
      transition,
      actions
    }
  };
  return makeEnv
};






// ============================ //
// ========= Policies ========= //
// ============================ //

var randPolicy = function(env) {
  var policy = function(state) {
    var actions = env.actions;
    return uniformDraw(actions(state));
  };
  return policy
};

var fixedNumPolicy = function(env, nObs) {
  var numObserved = function(state) {
    sum(map(function(n) {n != UNKNOWN},
            _.values(state)))
  };
  var policy = function(state) {
    if (numObserved(state) >= nObs) return TERM_ACTION;
    else {
      var options = unobservedNodes(state);
      return options.length ? uniformDraw(options) : TERM_ACTION
    }
  };
  return policy
};

// Enumerates all possible sequences of actions and outcomes, exactly
// computing the optimal policy. It is too slow to run on the full Mouselab
// environment.
var enumPolicy = function(opts) {
  var params = extend({
    maxExecutions: Infinity,
    myActions: actions
  }, opts);
  var myActions = params.myActions

  var actionAndValue = dp.cache(function(state) {
    var Q = function(action) {
      expectation(Infer({model() {
        var newState = transition(state, action);
        reward(state, action) + V(newState)
      }, method: 'enumerate'}))
    };
    if (myActions(state).length == 0) error('no actions');
    var result = maxWith(Q, myActions(state));
    if (result[0] == -Infinity) error('problem!\n'+myActions(state));
    return result;
  });

  var V = utils.cache(function(state) {
    state == TERM_STATE ? 0 : actionAndValue(state)[1]
  }, env);

  var policy = function(state) {
    var a = actionAndValue(state)[0]
    return a
  };
  return policy
  
};

var maxQPolicy = function(env, Q) {
  var policy = function(state) {
    // maxWith(f, xs) returns [x, f(x)] such that f(x) is maximized.
    var actions = env.actions;
    var actVal = maxWith(function(a) {Q(state, a)}, actions(state))
    return actVal[0]
  };
  return policy
};

// Takes an action with probability proportional to 
// exp(alpha * Q(state, action))
var softmaxQPolicy = function(env, Q, alpha) {
  var alpha = alpha || 1
  var policy = function(state) {
    var actions = env.actions;
    var a = uniformDraw(actions(state));
    factor(alpha * Q(state, a))
    return a
  };
  return policy
};


// // ============================ //
// // ========= Simulate ========= //
// // ============================ //

// var REWARD = Categorical({vs: [-20.0, -12.0, -4.0, 4.0, 12.0, 20.0],
//                           ps: [0.055, 0.157, 0.288, 0.288, 0.157, 0.055]});
// var COST = -1


// var simulate = function(policy, trueRewards, initialState, initialAction) {
//   var env = makeEnv(trueRewards);
//   var transition = env.transition;
//   var reward = env.reward;
//   var initialState = initialState || env.initialState;
//   var rec = function(acc) {
//     var state = _.last(acc.states);
//     if (state == TERM_STATE) return acc
//     else {
//       var action = (initialAction != undefined && acc.actions.length == 0 ?
//                     initialAction : policy(state))
//       if (action == -Infinity) error('Bad action.')
//       var newState = transition(state, action);
//       var r = reward(state, action);
//       return rec({
//         states: acc.states.concat([newState]),
//         rewards: acc.rewards.concat([r]),
//         actions: acc.actions.concat([action])
//       })
//     }
//   };
//   return rec({states: [initialState], rewards: [], actions: []})
// };