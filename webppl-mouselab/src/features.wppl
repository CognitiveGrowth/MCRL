// Defines features for linear approximation of the optimal Q function.

/* The state is represented as a tree. Each element in the tree is one of
   - a number, a previously observed reward
   - 'hidden', an unobserved node
   - 'observe', a node that will hypothetically be observed
*/

var OBSERVE = '__OBSERVE__';

/* The subjective reward function returns a sample from the
   distribution of the expected reward of the root of the given tree.
   that is about to be observed, the expected observationValue will be
   exactly the observationValue that is observed, and thus the future 
   expected reward has the same distribution as the current 
   unknown reward.
*/
var subjectiveReward = function(tree) {
  // tree is a 2-item list [val, [c1, c2...]] where each child c is also a tree.
  var val = tree[0];
  val == UNKNOWN ? expectation(globalStore.reward) :
  val == OBSERVE ? sample(globalStore.reward) :
  val  // already observed
};

/* The distribution over the expected value of the best path through 
   a tree, where the expectation is conditioned on the outcome of
   some yet-to-be-made observations.
*/
var observationValue = dp.cache(function(tree, params) {
    var params = extend({method: 'enumerate'}, params)
    Infer(extend(params, {model() {
      var bestChildVal = (
        tree[1].length == 0 ?
        0 :
        _.max(map(function(child) {sample(observationValue(child, params))},
                  tree[1]))
      );
      subjectiveReward(tree) + bestChildVal
    }}))
});  

var expectedObservationValue = function(tree, params) {
  // console.log('EOV', JSON.stringify(tree))
  var startTime = Date.now()
  var result = expectation(observationValue(tree, params))
  // console.log('observationValue:', Math.round(result * 100) / 100,
  //             '  time:', Date.now() - startTime)
  result
};

/* Alternative representation of the state, used to compute observationValue. */
var stateTree = function(state) {
  var s = state;
  [s[0], [
    // [s[1], [[s[5], [[s[9], []], [s[10], []]]]]],
    // [s[2], [[s[6], [[s[11], []], [s[12], []]]]]],
    // [s[3], [[s[7], [[s[13], []], [s[14], []]]]]],
    // [s[4], [[s[8], [[s[15], []], [s[16], []]]]]]
    [s[1], [[s[2], [[s[3], []], [s[4], []]]]]],
    [s[5], [[s[6], [[s[7], []], [s[8], []]]]]],
    [s[9], [[s[10], [[s[11], []], [s[12], []]]]]],
    [s[13], [[s[14], [[s[15], []], [s[16], []]]]]]
  ]]
}

var actionGroups = [
  [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4],
  [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7, 8],
  [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11, 12],
  [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15, 16]
]

var obsTree = function(state, toObserve) {
  stateTree(mapIndexed(function(i, r) {
    (_.includes(toObserve, i) && r == UNKNOWN) ? OBSERVE : r
  }, state))
}

// ============================ //
// ========= Features ========= //
// ============================ //

var termValue = cache(function(state) {
  // console.log('termValue')
  expectedObservationValue(stateTree(state))
});

var VOC_1 = function(state, action) {
  // console.log('VOC_1', action)
  expectedObservationValue(obsTree(state, [action])) - termValue(state)
}

var VPI_full = cache(function(state) {
  // console.log('VPI_full')
  expectedObservationValue(obsTree(state, nodes)) - termValue(state)
});

var VPI_action = function(state, action) {
  // console.log('VPI_action', action)
  var obs = actionGroups[action];
  expectedObservationValue(obsTree(state, obs)) - termValue(state)
};

// =================================== //
// ========= Learning Q_meta ========= //
// =================================== //

var dot = function(x, y) {
  sum(map2(function(x, y) {
    x * y
  }, x, y))
  
};
var cumsum = function (xs) {
  var acf = function (n, acc) {
    acc.concat((acc.length > 0 ? _.last(acc) : 0) + n)
  }
  reduce(acf, [], xs.reverse());
}

// Work-in-progress learning Q function by Monte Carlo regression.
var inferWeights = function() {
  // This should be a list of human stateAction pairs.
  var stateActions = [[env.initialState, 1]]
  Infer({model() {
    // Sample weights and create Q function and softmax Q policy.
    var weights = repeat(5, function() {gaussian(0, 2)});
    var Q = makeQ_meta(weights);
    var policy = softmaxQPolicy(Q, 1);
    var sigma = gamma(1, 1); // sd of empirical Q distribution

    // Observe 5 rollouts for the sampled weights.
    console.log('-------------------')
    console.log(weights)
    var _ = repeat(1, function() {
      console.log('start run')
      // Sample an empirical state and action, then execute it.
      var sa = uniformDraw(stateActions);
      var state = sa[0], action = sa[1];
      // TODO: Does inference preferentially weight the high-probability transitions?
      var newState = transition(state, action);
      console.log('outcome', newState[action])
      var r = reward(state, action);

      // Follow the softmaxQ policy after that initial action.
      var rollout = simulate(policy, newState);
      var trueQs = cumsum(rollout.rewards).reverse();

      // Observe the empirical Q for the sampled state and action.
      // This could be repeated to upweight the importance of the
      // empirical state actions in the regression.
      var predictQ = Q(state, action);
      var trueQ = r + trueQs[0];
      observe(Gaussian({mu: trueQ, sigma}), predictQ)
      console.log('predict', predictQ, ' observe', trueQ)

      // Observe the empirical Qs for the remainder of the rollout.
      map(function(i) {
        var predictQ = Q(rollout.states[i], rollout.actions[i]);
        observe(Gaussian({mu: trueQs[i], sigma}), predictQ);
      }, _.range(trueQs.length))
    });
    return {weights, sigma}
  },
  // method: 'SMC', particles: 100, rejuvSteps: 1
  method: 'SMC', particles: 3
  // method: 'MCMC', kernel: 'HMC', samples: 1000, onlyMAP: true
  })
};

var makeQ_meta = function(weights) {
  var Q_meta = cache(function(state, action) {
    // console.log('Q_meta', action)
    if (action == TERM_ACTION) {
      return termValue(state)
    }
    else {
      var features = [
        globalStore.cost,
        VOC_1(state, action),
        VPI_action(state, action),
        VPI_full(state),
        termValue(state)
      ]
      // console.log(features)
      return dot(weights, features)
    }
  });
  return Q_meta  
};

var Q_meta = makeQ_meta([-0.034821394751660226, 4.177078845688222, 1.216868835535616, 6.726366072163112, -1.1604972774449753]);


var V_meta = function(state) {
  var best = maxWith(function(action) {Q_meta(state, action)},
                     actions(state))
  console.log('best action', best[0], 'value', best[1])
  return best[1]
};

var calculatePR = function(arg) {
  console.log('calculatePR', arg)
  Q_meta(arg.state, arg.action) - V_meta(arg.state)
}
