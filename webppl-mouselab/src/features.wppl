// Defines features for linear approximation of the optimal Q function.

/* The state is represented as a tree. Each element in the tree is one of
   - a number, a previously observed reward
   - 'hidden', an unobserved node
   - 'observe', a node that will hypothetically be observed
*/

var OBSERVE = '__OBSERVE__';

globalStore.reward = Categorical({vs: [-20.0, -12.0, -4.0, 4.0, 12.0, 20.0],
                          ps: [0.055, 0.157, 0.288, 0.288, 0.157, 0.055]});

var env = mouselabProblem({reward: globalStore.reward, cost: 0})();
var actions = env.actions
/* The subjective reward function returns a sample from the
   distribution of the expected reward of the root of the given tree.
   that is about to be observed, the expected observationValue will be
   exactly the observationValue that is observed, and thus the future 
   expected reward has the same distribution as the current 
   unknown reward.
*/
var subjectiveReward = function(tree) {
  // tree is a 2-item list [val, [c1, c2...]] where each child c is also a tree.
  var val = tree[0];
  val == UNKNOWN ? expectation(globalStore.reward) :
  val == OBSERVE ? sample(globalStore.reward) :
  val  // already observed
};

/* The distribution over the expected value of the best path through 
   a tree, where the expectation is conditioned on the outcome of
   some yet-to-be-made observations.
*/
var observationValue = dp.cache(function(tree, params) {
    var params = extend({method: 'enumerate'}, params)
    Infer(extend(params, {model() {
      var bestChildVal = (
        tree[1].length == 0 ?
        0 :
        _.max(map(function(child) {sample(observationValue(child, params))},
                  tree[1]))
      );
      subjectiveReward(tree) + bestChildVal
    }}))
});

var expectedObservationValue = function(tree, params) {
  // console.log('EOV', JSON.stringify(tree))
  var startTime = Date.now()
  var result = expectation(observationValue(tree, params))
  // console.log('observationValue:', Math.round(result * 100) / 100,
  //             '  time:', Date.now() - startTime)
  result
};

/* Alternative representation of the state, used to compute observationValue. */
var stateTree = function(state) {
  var s = state;
  [s[0], [
    // [s[1], [[s[5], [[s[9], []], [s[10], []]]]]],
    // [s[2], [[s[6], [[s[11], []], [s[12], []]]]]],
    // [s[3], [[s[7], [[s[13], []], [s[14], []]]]]],
    // [s[4], [[s[8], [[s[15], []], [s[16], []]]]]]
    [s[1], [[s[2], [[s[3], []], [s[4], []]]]]],
    [s[5], [[s[6], [[s[7], []], [s[8], []]]]]],
    [s[9], [[s[10], [[s[11], []], [s[12], []]]]]],
    [s[13], [[s[14], [[s[15], []], [s[16], []]]]]]
  ]]
}

var relevantNodes = [
  [0], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3], [1, 2, 4],
  [5, 6, 7, 8], [5, 6, 7, 8], [5, 6, 7], [5, 6, 8],
  [9, 10, 11, 12], [9, 10, 11, 12], [9, 10, 11], [9, 10, 12],
  [13, 14, 15, 16], [13, 14, 15, 16], [13, 14, 15], [13, 14, 16],
]

var obsTree = function(state, toObserve) {
  stateTree(mapIndexed(function(i, r) {
    (_.includes(toObserve, i) && r == UNKNOWN) ? OBSERVE : r
  }, state))
}

// ============================ //
// ========= Features ========= //
// ============================ //

var termValue = cache(function(state) {
  expectedObservationValue(stateTree(state))
});

var VOC_1 = function(state, action) {
  expectedObservationValue(obsTree(state, [action])) - termValue(state)
}

var VPI_full = cache(function(state) {
  expectedObservationValue(obsTree(state, _.range(state.length))) - termValue(state)
});

var VPI_action = function(state, action) {
  var obs = relevantNodes[action];
  expectedObservationValue(obsTree(state, obs)) - termValue(state)
};

// =================================== //
// ========= Learning Q_meta ========= //
// =================================== //

var dot = function(x, y) {
  sum(map2(function(x, y) {
    x * y
  }, x, y))
  
};
var cumsum = function (xs) {
  var acf = function (n, acc) {
    acc.concat((acc.length > 0 ? _.last(acc) : 0) + n)
  }
  reduce(acf, [], xs.reverse());
}



var makeQ_meta = function(weights) {
  var Q_meta = cache(function(state, action) {
    if (action == TERM_ACTION) {
      return termValue(state)
    }
    else {
      var features = [
        globalStore.cost,
        VOC_1(state, action),
        VPI_action(state, action),
        VPI_full(state),
        termValue(state)
      ]
      return dot(weights, features)
    }
  });
  return Q_meta  
};

var makePRinfo = function(weights) {
  var Q_meta = makeQ_meta(weights);

  var PRinfo = function(arg) {
    if (arg.state == undefined) error('state undefined');
    if (arg.action == undefined) error('action undefined');
    var validActions = actions(arg.state)
    var Qs = _.fromPairs(map(function(action) {
      [action, Q_meta(arg.state, action)]
    }, validActions));
    var actVal = maxWith(function(action) {
      Qs[action]
    }, validActions);
    return {
      Qs,
      Q: Qs[arg.action],
      bestAction: actVal[0],
      V: actVal[1]
    }
  };
  return PRinfo
};
