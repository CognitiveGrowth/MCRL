/* This method requires a discrete reward distribution. Things slow down as you
   add more possible rewards.
*/
var values = [-2, 1, 0, 1, 2, 5]
var norm = Gaussian({mu:0, sigma: 5});
var prob = function(r) {Math.exp(norm.score(r))}
var REWARD = Categorical({ps: map(prob, values), vs: values})


/* The state is represented as a tree. Each element in the tree is one of
   - a number, a previously observed reward
   - 'hidden', an unobserved node
   - 'observe', a node that will hypothetically be observed
*/
var value = function(node) {
    return node[0]
};
var children = function(node) {
    return node[1]
};
var ___ = 'hidden';
var _O_ = 'observe';
var initial_state = [0,
 [[___, [[___, [[___, []], [___, []]]]]],
  [___, [[___, [[___, []], [___, []]]]]],
  [___, [[___, [[___, []], [___, []]]]]],
  [___, [[___, [[___, []], [___, []]]]]]]]


/* The subjective reward function returns a sample from the
   distribution of the expected reward of a node. For a node
   that is about to be observed, the expected observation_value will be
   exactly the observation_value that is observed, and thus the future 
   expected reward has the same distribution as the current 
   unknown reward.
*/
var reward = function(node) {
  var v = value(node);
  v == 'hidden' ? expectation(REWARD) :
  v == 'observe' ? sample(REWARD) :
  v  // already observed
};


/* The distribution over the expected value of the best path through 
   a tree, where the expectation is conditioned on the outcome of
   some yet-to-be-made observations.
*/
var observation_value = dp.cache(function(node, params) {
    var prm = (params == undefined) ? {} : params
    Infer(extend(prm, {model() {
        var best_child_val = (children(node).length == 0 ?
                              0 :
                              _.max(map(function(child) {sample(observation_value(child))},
                                        children(node))));
        reward(node) + best_child_val
    }}))
});  

var expected_observation_value = function(node, params) {
  var start_time = Date.now()
  var result = expectation(observation_value(node, params))
  console.log('observation_value:', Math.round(result * 100) / 100, '  time:', Date.now() - start_time)
  return result
};


// ========== DEMOS ========== //

var first_click_demo = function(args) {
  console.log('\n=== Consider clicking on a depth-1 node ===')

  console.log('observation_value of acting without observation')
  expected_observation_value(initial_state)

  console.log('\nmyopic VOC click depth 1')
  expected_observation_value([0,
   [[_O_, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]]]])

  console.log('\nVPI action click depth 1')
  expected_observation_value([0,
   [[_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]]]])

  console.log('\nVPI full')
  expected_observation_value([0,
   [[_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]]]])
  return null
};

var second_click_demo = function(args) {
  console.log('\n=== Value of a depth-1 node is 2, consider clicking another depth-1 node ===')

  console.log('observation_value of acting without observation')
  expected_observation_value([0,
   [[ 2 , [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]]]])
  
  console.log('\nmyopic VOC click another depth 1')
  expected_observation_value([0,
   [[ 2 , [[___, [[___, []], [___, []]]]]],
    [_O_, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]]]])

  console.log('\nVPI action click another depth 1')
  expected_observation_value([0,
   [[ 2 , [[___, [[___, []], [___, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]],
    [___, [[___, [[___, []], [___, []]]]]]]])

  console.log('\nVPI full')
  expected_observation_value([0,
   [[ 2 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]]]])    
};

var vpi_demo = function(args) {
  console.log('\n=== VPI full for clicking all depth-1 nodes ===')
  expected_observation_value([0,
   [[_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]]]])

  expected_observation_value([0,
   [[ 1 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]]]])

  expected_observation_value([0,
   [[ 1 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [ 2 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]]]])

  expected_observation_value([0,
   [[ 1 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [ 2 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [ 0 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [_O_, [[_O_, [[_O_, []], [_O_, []]]]]]]])

  expected_observation_value([0,
   [[ 1 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [ 2 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [ 0 , [[_O_, [[_O_, []], [_O_, []]]]]],
    [ 5 , [[_O_, [[_O_, []], [_O_, []]]]]]]])
};

first_click_demo()
second_click_demo()
vpi_demo()

null