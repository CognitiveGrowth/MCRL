{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredcallaway/miniconda3/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.3)\n",
    "\n",
    "from agents import Agent\n",
    "from mouselab import MouselabEnv\n",
    "from distributions import Normal, Categorical\n",
    "from policies import FixedPlanPolicy\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_env(cost, ground_truth=False, initial_states=None):\n",
    "    \"\"\"Returns a MouselabEnv with branching [4,1,2].\n",
    "    \n",
    "    If `ground_truth` is True, the reward observed at a given node will be\n",
    "    constant across runs on this env. This reduces variance of the return.\"\"\"\n",
    "    reward = Normal(0, 10).to_discrete(6)\n",
    "    env = MouselabEnv([4,1,2], reward=reward, cost=cost, initial_states=initial_states)\n",
    "    if ground_truth:\n",
    "        env.ground_truth = np.array([0, *reward.sample(len(env.tree) - 1)])\n",
    "    return env\n",
    "\n",
    "def make_envs(cost, n=100, ground_truth=None, initial_states=None):\n",
    "    # Note, ground_truth can be an int in which case it acts as a random seed.\n",
    "    if ground_truth is not None:\n",
    "        np.random.seed(ground_truth)\n",
    "        return [make_env(cost, True, initial_states) for _ in range(n)]\n",
    "    else:\n",
    "        return [make_env(cost, False, initial_states)] * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the LC policy on environments with random reward structures.\n",
    "However, to ensure that the policy is near-optimal on belief states\n",
    "that participants find themselves in, the initial state is drawn from\n",
    "the empirical belief-state distribution of human participants in the\n",
    "no-feedback condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def read_state_actions():\n",
    "    with open('data/state_actions.json') as f:\n",
    "        data = json.load(f)\n",
    "    result = {}\n",
    "    for cost in data:\n",
    "        result[float(cost)] = r = {}\n",
    "        env = make_env(float(cost))\n",
    "        def parse_state(state):\n",
    "            return tuple(env.reward if x == '__' else float(x)\n",
    "                  for x in state)\n",
    "        def parse_action(action):\n",
    "            return env.term_action if action == '__TERM_ACTION__' else action\n",
    "        r['states'] = list(map(parse_state, data[cost]['states']))\n",
    "        r['actions'] = list(map(parse_action, data[cost]['actions']))\n",
    "    return result\n",
    "\n",
    "state_actions = read_state_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LC policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/cross_val_policies.pkl\n"
     ]
    }
   ],
   "source": [
    "import skopt\n",
    "import warnings\n",
    "from evaluation import *\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        message=\"The objective has been evaluated at this point before.\")\n",
    "\n",
    "COSTS = state_actions.keys()\n",
    "N_TRAIN = 500\n",
    "N_CROSS_VAL = 2000\n",
    "N_CALLS = 40\n",
    "NORMALIZE = True\n",
    "\n",
    "def filename(cost):\n",
    "    c = round(float(cost), 5)\n",
    "    return 'data/421_{}'.format(c)\n",
    "    \n",
    "        \n",
    "def write_bo_policy(cost):\n",
    "    empirical_states = state_actions[cost]['states']\n",
    "    train_envs = make_envs(cost, initial_states=empirical_states, n=N_TRAIN)\n",
    "    pol, result = bo_policy(train_envs, max_cost=len(train_envs[0].tree),\n",
    "                            normalize_voi=True, n_random_starts=10,\n",
    "                            n_calls=N_CALLS, n_jobs=25, return_result=True,)\n",
    "    fn = filename(cost)\n",
    "    result.specs['args'].pop('func')  # can't pickle\n",
    "    result.specs['info'] = {\n",
    "        'cost': cost,\n",
    "        'n_train': N_TRAIN,\n",
    "        'n_calls': N_CALLS,\n",
    "        'theta': pol.theta\n",
    "    }\n",
    "    skopt.dump(result, fn + '.pkl')\n",
    "    np.save(fn + '.npy', pol.theta)\n",
    "    return result\n",
    "\n",
    "def read_bo_policy(cost, cross_val=True):\n",
    "    result = read_bo_result(cost)\n",
    "    empirical_states = state_actions[cost]['states']\n",
    "    envs = make_envs(cost, initial_states=empirical_states, n=N_CROSS_VAL)\n",
    "    if cross_val:\n",
    "        n_consider = 5\n",
    "        idx = result.func_vals.argsort()[:n_consider]\n",
    "        top_x = np.array(result.x_iters)[idx]\n",
    "        top_theta = [x2theta(x, True) for x in top_x]\n",
    "        theta = max(top_theta, key=\n",
    "                    lambda th: get_util(LiederPolicy(th), envs, parallel=Parallel(20)))\n",
    "    else:\n",
    "        return LiederPolicy(result.specs['info']['theta'])\n",
    "    return LiederPolicy(theta)\n",
    "\n",
    "def read_bo_result(cost):\n",
    "    return skopt.load(filename(cost) + '.pkl')\n",
    "\n",
    "import joblib\n",
    "cv_file = 'data/cross_val_policies.pkl'\n",
    "try:\n",
    "    policies = joblib.load(cv_file)\n",
    "    print('Loaded', cv_file)\n",
    "except FileNotFoundError:\n",
    "    print('Training LC policies')\n",
    "    for c in COSTS:\n",
    "        write_bo_policy(c)\n",
    "    policies = {c: read_bo_policy(c, cross_val=True) for c in COSTS}\n",
    "    print('Running cross validation')\n",
    "    joblib.dump(policies, cv_file)\n",
    "    \n",
    "\n",
    "for k, v in policies.items():\n",
    "    print(k, v.theta.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned feature weights:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COST</th>\n",
       "      <th>VOI_1</th>\n",
       "      <th>VPI_a</th>\n",
       "      <th>VPI_full</th>\n",
       "      <th>TERM_REWARD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>4.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.00</th>\n",
       "      <td>14.91</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       COST  VOI_1  VPI_a  VPI_full  TERM_REWARD\n",
       "cost                                            \n",
       "0.25   1.00   0.00   0.94      0.06          1.0\n",
       "1.00   4.45   0.00   0.49      0.51          1.0\n",
       "4.00  14.91   0.32   0.11      0.57          1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Learned feature weights:')\n",
    "pd.DataFrame(\n",
    "    [[cost, *pol.theta.round(2)] for cost, pol in policies.items()],\n",
    "    columns='cost COST VOI_1 VPI_a VPI_full TERM_REWARD'.split()\n",
    ").set_index('cost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empirical states</th>\n",
       "      <th>rewards</th>\n",
       "      <th>util</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>random</td>\n",
       "      <td>19.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>random</td>\n",
       "      <td>19.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>experimental</td>\n",
       "      <td>18.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>experimental</td>\n",
       "      <td>19.343750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   empirical states       rewards       util\n",
       "0             False        random  19.677500\n",
       "1              True        random  19.064500\n",
       "2             False  experimental  18.734375\n",
       "3             False  experimental  19.343750"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation():\n",
    "    for cost in COSTS:\n",
    "        empirical_states = state_actions[cost]['states']\n",
    "        yield {'cost': cost,\n",
    "               'rewards': 'random',\n",
    "               'empirical states': False,\n",
    "               'util': get_util(pol, make_envs(cost, n=500))}\n",
    "\n",
    "        yield {'cost': cost,\n",
    "               'rewards': 'random',\n",
    "               'empirical states': True,\n",
    "               'util': get_util(pol, make_envs(cost, n=500,\n",
    "                                               initial_states=empirical_states))}\n",
    "        yield {'cost': cost,\n",
    "               'rewards': 'experimental',\n",
    "               'empirical states': False,\n",
    "               'util': get_util(pol, make_envs(cost, n=16, ground_truth=1))}\n",
    "\n",
    "        yield {'cost': cost,\n",
    "               'rewards': 'experimental',\n",
    "               'empirical states': True,\n",
    "               'util': get_util(pol, make_envs(cost, n=16, ground_truth=1, \n",
    "                                               initial_states=empirical_states))}\n",
    "    \n",
    "pd.DataFrame(evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost = 0.25:   0%|          | 0/3868 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 2/3868 [00:11<6:01:08,  5.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 4/3868 [00:19<5:36:56,  5.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 6/3868 [00:29<5:29:57,  5.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 7/3868 [00:41<7:34:28,  7.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 8/3868 [00:49<8:06:25,  7.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 9/3868 [00:57<7:58:46,  7.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 11/3868 [01:07<7:14:30,  6.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "cost = 0.25:   0%|          | 13/3868 [01:18<6:46:34,  6.33s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from toolz import memoize\n",
    "from tqdm import trange\n",
    "def get_qs(cost, pol):\n",
    "    env = make_env(cost)\n",
    "    parallel = Parallel(20)\n",
    "    def V(state, rollouts_per_state=100):\n",
    "        env = make_env(cost, initial_states=[state])\n",
    "        if state == env.term_state:\n",
    "            return 0\n",
    "        return get_util(pol, [env] * rollouts_per_state, parallel=parallel)\n",
    "\n",
    "    def Q(state, action):\n",
    "        return sum(p * (r + V(s1))\n",
    "                   for p, s1, r in env.results(state, action))\n",
    "\n",
    "    states = state_actions[cost]['states']\n",
    "    actions = state_actions[cost]['actions']\n",
    "    qs = []\n",
    "    for i in trange(len(states), desc='cost = {}'.format(cost)):\n",
    "        qs.append(Q(states[i], actions[i]))\n",
    "    return qs       \n",
    "\n",
    "for c, p in policies.items():\n",
    "    joblib.dump(get_qs(c, p), 'data/qs_{}'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qs = {c: joblib.load(f'data/qs_{c}') for c in COSTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-534c607d1f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mLQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiederQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "class LiederQ(object):\n",
    "    \"\"\"docstring for LiederQ\"\"\"\n",
    "    def __init__(self, env, theta):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.theta = theta\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        if action == self.env.term_action:\n",
    "            return self.env.expected_term_reward(state)\n",
    "        else:\n",
    "            return np.dot(self.theta, self.env.action_features(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.25,  rms = 1.8802111434953652\n",
      "cost = 1.0,  rms = 1.5130383481309841\n",
      "cost = 4.0,  rms = 0.805113102971472\n"
     ]
    }
   ],
   "source": [
    "@memoize\n",
    "def get_features(cost):\n",
    "    env = make_env(cost)\n",
    "    return [[1, *env.action_features(a, s)]\n",
    "            for s, a in zip(*state_actions[cost].values())]\n",
    "    \n",
    "\n",
    "def regress_q(cost):\n",
    "    X = np.stack(get_features(cost))\n",
    "    y = np.array(qs[cost])\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    rms = np.sqrt(np.mean(((X @ beta) - y) ** 2))\n",
    "    print(f'cost = {cost},  rms = {rms}')\n",
    "    return beta\n",
    "\n",
    "\n",
    "betas = {c: regress_q(c) for c in COSTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.25': [0.21206923649241527,\n",
      "          0.9597756744079762,\n",
      "          -0.6976333377042633,\n",
      "          0.16087680964708706,\n",
      "          0.8794170837427167,\n",
      "          0.9854813263869885],\n",
      " '1.00': [0.1634283077172673,\n",
      "          2.004429571617946,\n",
      "          -0.4840571944907661,\n",
      "          0.11101666806487959,\n",
      "          0.632190859367936,\n",
      "          0.9886360506389685],\n",
      " '4.00': [0.011900418547917083,\n",
      "          0.8678029706323578,\n",
      "          0.1822980332672186,\n",
      "          0.0041098709999925764,\n",
      "          0.11509907015336761,\n",
      "          0.9977587838037578]}\n"
     ]
    }
   ],
   "source": [
    "with open('data/q_weights.json', 'w+') as f:\n",
    "    x = {f'{c:.2f}': beta.tolist() for c, beta in betas.items()}\n",
    "    pprint(x)\n",
    "    json.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "logp = -7,506.5, ||grad|| = 34.802: 100%|██████████| 53/53 [00:00<00:00, 2310.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VOI': array([  3.694e-08,   2.422e-07,   5.172e-01]),\n",
      " 'VOI_logodds__': array([-17.114, -15.234,   0.069]),\n",
      " 'cost': array([ 1.]),\n",
      " 'cost_interval__': array([-15.352]),\n",
      " 'sigma': array(1.9136456349219368),\n",
      " 'sigma_log__': array(0.6490101321706021)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "def regress(cost):\n",
    "    X = np.stack(get_features(cost))\n",
    "    y = np.array(qs[cost])\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        beta = tt.concatenate([\n",
    "#             pm.Exponential('intercept', 1, shape=(1,), testval=1e-10),\n",
    "#             pm.Exponential('cost', 1, shape=(1,), testval=1.),\n",
    "#             pm.Exponential('VOI', 3, shape=(3,), testval=1/3),\n",
    "            tt.zeros(1),\n",
    "            pm.Uniform('cost', 1, 16, shape=(1,)),\n",
    "            pm.Beta('VOI', 1, 1, shape=(3,)),\n",
    "            tt.ones(1)\n",
    "        ])\n",
    "#         sigma = pm.HalfCauchy('sigma', beta=10, testval=1.)\n",
    "        sigma = pm.Exponential('sigma', 100\n",
    "        likelihood = pm.Normal('likelihood', mu=tt.dot(X, beta), sd=sigma,\n",
    "                               observed=y)\n",
    "        \n",
    "#         trace = pm.sample(500, njobs=20, progressbar=False, tune=1000)\n",
    "        trace = None\n",
    "        MAP = pm.find_MAP()\n",
    "    return model, trace, MAP\n",
    "        \n",
    "\n",
    "model, trace, MAP = regress(1.00)\n",
    "pprint(MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VOI_1': array([  2.592e-12]),\n",
      " 'VOI_1_logodds__': array([-26.679]),\n",
      " 'VOI_a': array([  1.228e-09]),\n",
      " 'VOI_a_logodds__': array([-20.518]),\n",
      " 'VOI_full': array([ 0.765]),\n",
      " 'VOI_full_logodds__': array([ 1.179]),\n",
      " 'cost': array([ 1.]),\n",
      " 'cost_interval__': array([-15.155]),\n",
      " 'sigma': array(3.1192498095758183),\n",
      " 'sigma_log__': array(1.1375925272609253)}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(MAP)\n",
    "# pm.traceplot(trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "def test_pols(test_envs, policies):\n",
    "    def test():\n",
    "        # print('test', cost)\n",
    "        for name, policy in policies.items():\n",
    "            df = evaluate(policy, envs=test_envs)\n",
    "            df['agent'] = name\n",
    "            # df['depth'] = depth\n",
    "            # df['cost'] = cost\n",
    "            yield df\n",
    "    df = pd.concat(test())\n",
    "    print('done', depth, cost)\n",
    "    return df\n",
    "\n",
    "# with Parallel(n_jobs=48) as parallel:\n",
    "#     data = parallel(delayed(run_params)(depth, cost)\n",
    "#                     for cost, depth in bo_policies.keys()\n",
    "# #                     for depth in range(2, 6)\n",
    "# #                     for cost in np.logspace(-3, 0, 12)\n",
    "#                    )\n",
    "# #     df = pd.concat(data)\n",
    "df = run_params(4, .05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "71px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
