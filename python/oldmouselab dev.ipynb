{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import itertools as it\n",
    "from distributions import cmax, smax, expectation, Normal, PointMass, SampleDist\n",
    "from toolz import memoize\n",
    "import random\n",
    "from contracts import contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO = PointMass(0)\n",
    "\n",
    "class OldMouselabEnv(gym.Env):\n",
    "    \"\"\"MetaMDP for the Mouselab task.\"\"\"\n",
    "    \n",
    "    term_state = '__term_state__'\n",
    "    def __init__(self, gambles=4, attributes=5, reward=None, cost=0,\n",
    "                 ground_truth=None, initial_states=None, randomness=1):\n",
    "        \n",
    "        self.gambles = gambles # no of gambles\n",
    "        \n",
    "        # distribution and number of attributes\n",
    "        if hasattr(attributes, '__len__'):\n",
    "            self.outcomes = len(attributes)\n",
    "            self.dist = np.array(attributes)/np.sum(attributes)\n",
    "        else:\n",
    "            self.outcomes = attributes\n",
    "            self.dist = np.random.dirichlet(np.ones(attributes)*randomness,size=1)\n",
    "\n",
    "        # reward for the payoffs\n",
    "        self.reward = reward if reward is not None else Normal(1, 1)\n",
    "        if hasattr(reward, 'sample'):\n",
    "            self.iid_rewards = True\n",
    "        else:\n",
    "            self.iid_rewards = False\n",
    "            \n",
    "        self.cost = - abs(cost)\n",
    "        self.ground_truth = np.array(ground_truth) if ground_truth is not None else None\n",
    "        self.grid = np.arange(self.gambles*self.outcomes).reshape((self.gambles, self.outcomes))\n",
    "        self.initial_states = initial_states\n",
    "        self.exact = hasattr(reward, 'vals')\n",
    "        if self.exact:\n",
    "            assert self.iid_rewards\n",
    "            self.max = cmax\n",
    "            self.init = np.array([self.reward,] * (self.gambles*self.outcomes))\n",
    "        else:\n",
    "            # Distributions represented as samples.\n",
    "            self.max = smax\n",
    "            self.init = np.array([self.reward.to_sampledist(),] * (self.gambles*self.outcomes))\n",
    "        self.sample_term_reward = False\n",
    "        self.term_action = self.gambles*self.outcomes + 1\n",
    "        self.reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        if self.initial_states:\n",
    "            self.init = random.choice(self.initial_states)\n",
    "        self._state = self.init\n",
    "        return self.features(self._state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._state is self.term_state:\n",
    "            assert 0, 'state is terminal'\n",
    "            # return None, 0, True, {}\n",
    "        if action == self.term_action:\n",
    "            # self._state = self.term_state\n",
    "            if self.sample_term_reward:\n",
    "                if self.ground_truth is not None:\n",
    "                    gamble = self.best_gamble()\n",
    "                    reward = self.ground_truth[gamble].sum()\n",
    "                else:\n",
    "                    reward = self.term_reward().sample()\n",
    "            else:\n",
    "                reward = self.term_reward().expectation()\n",
    "            done = True\n",
    "        elif not hasattr(self._state[action], 'sample'):  # already observed\n",
    "            assert 0, self._state[action]\n",
    "            reward = 0\n",
    "            done = False\n",
    "        else:  # observe a new node\n",
    "            self._state = self._observe(action)\n",
    "            reward = self.cost\n",
    "            done = False\n",
    "        return self.features(self._state), reward, done, {}\n",
    "\n",
    "    def _observe(self, action):\n",
    "        if self.ground_truth is not None:\n",
    "            result = self.ground_truth[action]\n",
    "        else:\n",
    "            result = self._state[action].sample()\n",
    "        s = list(self._state)\n",
    "        s[action] = result\n",
    "        return tuple(s)\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Yields actions that can be taken in the given state.\n",
    "\n",
    "        Actions include observing the value of each unobserved node and terminating.\n",
    "        \"\"\"\n",
    "        if state is self.term_state:\n",
    "            return\n",
    "        for i, v in enumerate(state):\n",
    "            if hasattr(v, 'sample'):\n",
    "                yield i\n",
    "        yield self.term_action\n",
    "\n",
    "    def results(self, state, action):\n",
    "        \"\"\"Returns a list of possible results of taking action in state.\n",
    "\n",
    "        Each outcome is (probability, next_state, reward).\n",
    "        \"\"\"\n",
    "        if action == self.term_action:\n",
    "            # R = self.term_reward()\n",
    "            # S1 = Categorical([self.term_state])\n",
    "            # return cross(S1, R)\n",
    "            yield (1, self.term_state, self.expected_term_reward(state))\n",
    "        else:\n",
    "            for r, p in state[action]:\n",
    "                s1 = list(state)\n",
    "                s1[action] = r\n",
    "                yield (p, tuple(s1), self.cost)\n",
    "\n",
    "    def features(self, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        return state\n",
    "\n",
    "\n",
    "    def action_features(self, action, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        assert state is not None\n",
    "\n",
    "\n",
    "        if action == self.term_action:\n",
    "            return np.array([\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                self.expected_term_reward(state)\n",
    "            ])\n",
    "\n",
    "        return np.array([\n",
    "            self.cost,\n",
    "            self.myopic_voc(action, state),\n",
    "            self.vpi_action(action, state),\n",
    "            self.vpi(state),\n",
    "            self.expected_term_reward(state)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def term_reward(self, state=None):\n",
    "        state = state if state is not None else self._state\n",
    "        assert state is not None\n",
    "        return self.state_value(0, state)\n",
    "    \n",
    "\n",
    "    def state_value(self, state=None):\n",
    "        \"\"\"A distribution over total rewards after the given node.\"\"\"\n",
    "        state = state if state is not None else self._state\n",
    "        grid = np.array(state).reshape(self.gambles,self.outcomes)\n",
    "        best_gamble = max((grid[g] for g in range(self.gambles)), default=ZERO, key=lambda x: sum(map(expectation,x)))\n",
    "        return np.sum(best_gamble)\n",
    "    \n",
    "    def expected_term_reward(self, state):\n",
    "        return self.term_reward(state).expectation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([SD(200), SD(200), SD(200), SD(200), SD(200), SD(200), SD(200),\n",
       "       SD(200), SD(200), SD(200), SD(200), SD(200), SD(200), SD(200),\n",
       "       SD(200), SD(200)], dtype=object)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gambles = 4\n",
    "attributes = [0.25,0.15,0.16,0.19]\n",
    "env = OldMouselabEnv(gambles, attributes)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env._state\n",
    "grid = np.array(state).reshape(env.gambles,env.outcomes)\n",
    "best_gamble = max((grid[g] for g in range(env.gambles)), default=ZERO, key=lambda x: sum(map(expectation,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([SD(200), SD(200), SD(200), SD(200)], dtype=object)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5330709939410223"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(best_gamble).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
